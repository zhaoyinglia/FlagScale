The following files may have been Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. in 2025.

CMakeLists.txt
benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
benchmarks/kernels/benchmark_moe.py
cmake/utils.cmake
csrc/activation_kernels.cu
csrc/attention/attention_kernels.cuh
csrc/attention/attention_utils.cuh
csrc/attention/dtype_float16.cuh
csrc/attention/paged_attention_v1.cu
csrc/attention/paged_attention_v2.cu
csrc/cache.h
csrc/cache_kernels.cu
csrc/core/scalar_type.hpp
csrc/cuda_compat.h
csrc/custom_all_reduce.cuh
csrc/cutlass_extensions/common.hpp
csrc/mamba/causal_conv1d/causal_conv1d.cu
csrc/mamba/mamba_ssm/selective_scan_fwd.cu
csrc/moe/moe_align_sum_kernels.cu
csrc/moe/moe_ops.h
csrc/ops.h
csrc/quantization/awq/Hgemm_nn_128x32x128_8m1n8k_awq.hpp
csrc/quantization/awq/awq_4bits.cuh
csrc/quantization/awq/dequant.cuh
csrc/quantization/awq/dequantize.cuh
csrc/quantization/awq/gemm_kernels.cu
csrc/quantization/awq/hgemv_nn_splitk_awq.hpp
csrc/quantization/awq/hgemv_selector.hpp
csrc/quantization/compressed_tensors/int8_quant_kernels.cu
csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh
csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
csrc/quantization/fp8/common.cu
csrc/quantization/fp8/nvidia/quant_utils.cuh
csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
csrc/quantization/fused_kernels/quant_conversions.cuh
csrc/quantization/gguf/ggml-common.h
csrc/quantization/gptq/Hgemm_common.cuh
csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-4bits.hpp
csrc/quantization/gptq/Hgemm_nn_128x32x128_8m1n8k_gptq-8bits.hpp
csrc/quantization/gptq/dequant.cuh
csrc/quantization/gptq/gptq.cuh
csrc/quantization/gptq/hgemm_gptq.h
csrc/quantization/gptq/hgemv_nn_splitk_gptq.hpp
csrc/quantization/gptq/hgemv_nn_splitk_gptq_int8.hpp
csrc/quantization/gptq/hgemv_selector.hpp
csrc/quantization/gptq/q_gemm.cu
csrc/quantization/gptq/qdq_4.cuh
csrc/quantization/gptq/scalar_type.hpp
csrc/quantization/vectorization.cuh
csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
examples/llm_engine_example_bytemlperf.py
requirements-cuda.txt
setup.py
tests/distributed/test_pynccl.py
tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
tests/kernels/test_awq.py
tests/kernels/test_causal_conv1d.py
tests/kernels/test_cutlass.py
tests/kernels/test_fused_quant_layernorm.py
tests/kernels/test_int8_quant.py
tests/kernels/test_mamba_ssm.py
tests/kernels/test_marlin_gemm.py
tests/kernels/utils.py
vllm/__init__.py
vllm/_custom_ops.py
vllm/attention/backends/flash_attn.py
vllm/attention/backends/flash_attn_pg.py
vllm/attention/backends/mla/utils.py
vllm/attention/backends/triton_mla.py
vllm/attention/layer.py
vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
vllm/attention/ops/blocksparse_attention/interface.py
vllm/attention/ops/paged_attn.py
vllm/attention/ops/triton_decode_attention.py
vllm/config.py
vllm/distributed/device_communicators/pynccl_wrapper.py
vllm/distributed/kv_transfer/kv_connector/base.py
vllm/distributed/kv_transfer/kv_connector/factory.py
vllm/distributed/kv_transfer/kv_connector/layerwise_connector.py
vllm/distributed/kv_transfer/kv_connector/simple_connector.py
vllm/distributed/kv_transfer/kv_lookup_buffer/simple_dict_buffer.py
vllm/distributed/kv_transfer/kv_transfer_agent.py
vllm/distributed/kv_transfer/kv_transfer_utils.py
vllm/distributed/parallel_state.py
vllm/distributed/utils.py
vllm/engine/arg_utils.py
vllm/engine/llm_engine.py
vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
vllm/envs.py
vllm/executor/executor_base.py
vllm/executor/mp_distributed_executor.py
vllm/executor/ray_distributed_executor.py
vllm/model_executor/layers/fused_moe/fused_moe.py
vllm/model_executor/layers/linear.py
vllm/model_executor/layers/logits_processor.py
vllm/model_executor/layers/quantization/__init__.py
vllm/model_executor/layers/quantization/awq.py
vllm/model_executor/layers/quantization/base_config.py
vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
vllm/model_executor/layers/quantization/gptq.py
vllm/model_executor/layers/quantization/moe_wna16.py
vllm/model_executor/layers/quantization/utils/fp8_utils.py
vllm/model_executor/layers/vocab_parallel_embedding.py
vllm/model_executor/model_loader/__init__.py
vllm/model_executor/model_loader/loader.py
vllm/model_executor/models/baichuan_moe.py
vllm/model_executor/models/deepseek.py
vllm/model_executor/models/deepseek_v2.py
vllm/model_executor/models/qwen.py
vllm/model_executor/models/registry.py
vllm/model_executor/models/telechat.py
vllm/platforms/__init__.py
vllm/platforms/cuda.py
vllm/platforms/pynvml.py
vllm/triton_utils/custom_cache_manager.py
vllm/utils.py
vllm/v1/attention/backends/flash_attn.py
vllm/v1/core/scheduler.py
vllm/v1/engine/core.py
vllm/v1/executor/abstract.py
vllm/v1/executor/ray_distributed_executor.py
vllm/version.py
vllm/worker/model_runner.py
vllm/worker/multi_step_model_runner.py
vllm/worker/worker_base.py
